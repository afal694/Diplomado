{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "686443e7-4a6a-4842-8cdd-b8b7e9da715b",
   "metadata": {},
   "source": [
    "## NVIDIA LLM NeMo MegaTron"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb535fa6-c3c9-43e5-a127-37866c10be9d",
   "metadata": {},
   "source": [
    "- Crecimiento en el mercado de procesadores de lenguaje natural \n",
    "- La industria está creando los  LLM, grandes NLP con inmensas cantidades de parámetros (cientos de millones, de billones de parámetros)\n",
    "- El lanzamiento del Open Pretrained Transformer, un modelo de lenguaje que ha transformado los NLP y AI\n",
    "- Nvidia hace unos años menciona que AI, ML y DL son claves en su crecimiento\n",
    "- Ujval Kapasi (VicePresident Nvidia): Cuando un NLP tiene cantidad de parámetros de cientos de billones a trillones, tienen un gran potencial para múltiples aplicaciones sin necesidad de reentrenarlos, como es usual en otros tipos de NN. \n",
    "- El reto es optimizar la arquitectura de las GPU's para el entrenamiento de LLM (grandes cantidades de parámetros) (vice Nvidia) en un tiempo razonable\n",
    "- Multiplicación de matrcies con Tensor Paralellism es relativamente eficiente, pero en cantidades grandes la capacidad de cómputo se vuelve considerable\n",
    "- SP (Sequence Paralellism) y SAR (selective Activation Recomputation) son nuevas técnicas que combinadas reducen la cantidad de memoria hasta por 5 veces\n",
    "- HyperParameter tool: Diseñada para automáticamente encontrar el correcto entrenamiento y configuraciones sin cambios de código.\n",
    "- NeMo Megatron: Un framkework LLM actualizado con estas dos nuevas técnicas para optimizar el tiempo de entrenamiento hasta en un 30%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7e31a59c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"./images/SP_MegaTron.png\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "# ![title](\"images/SP_MegaTron.png\")\n",
    "Image(url=\"./images/SP_MegaTron.png\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
